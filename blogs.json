{"status":"ok","feed":{"url":"https://medium.com/feed/@HaveASesh","title":"Stories by HaveASesh on Medium","link":"https://medium.com/@HaveASesh?source=rss-ccd6c29405e0------2","author":"","description":"Stories by HaveASesh on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/1*gAz4OrsTnxXQiEmtqEA6GA.jpeg"},"items":[{"title":"Standing Out in the Job Hunt: Building a Data Science Project That Matches the Job Description","pubDate":"2024-11-08 14:16:29","link":"https://medium.com/@HaveASesh/standing-out-in-the-job-hunt-building-a-data-science-project-that-matches-the-job-description-58a008076cdb?source=rss-ccd6c29405e0------2","guid":"https://medium.com/p/58a008076cdb","author":"HaveASesh","thumbnail":"","description":"\n<p>In today\u2019s competitive job market, applying for a job means more than just submitting a resume and cover letter. To truly stand out, it\u2019s essential to demonstrate real, hands-on skills that align with the role you\u2019re targeting. In my journey to apply for a data science position, I decided to go beyond the basics and develop a project that mirrors the responsibilities of the job description, enhancing my understanding of the role and showcasing my abilities.</p>\n<p>This article walks through my project step-by-step\u200a\u2014\u200afrom defining the problem, sourcing the data, performing exploratory data analysis, to building and validating a machine learning model. Whether I get the job or not, this method has provided me with invaluable experience, deeper insight into the role, and a practical showcase of my\u00a0skills.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*WK_iq9MKpTM7wZb5qmQZTw.png\"></figure><p>When applying for a data science position, especially in the maritime sector, I saw an opportunity to create a project focused on <strong>predicting fuel consumption for vessels</strong>. The goal was to use real-world data to model fuel consumption based on various features, such as vessel type, speed, and geolocation. This directly aligns with the job responsibilities, where predictive modeling and data analysis are\u00a0key.</p>\n<h3>Step 1: Setting Up the\u00a0Data</h3>\n<p>The first step in any data project is obtaining the right data. For this project, I used the <a href=\"https://coast.noaa.gov/\">NOAA dataset</a> that provides extensive records of vessel movements, speeds, and geolocation data. Here\u2019s how I approached this:</p>\n<h4>Code Snippet: Import Libraries and Load\u00a0Data</h4>\n<pre>import pandas as pd<br>import seaborn as sns<br>import matplotlib.pyplot as plt<br>import numpy as np<br>from scipy.stats import ks_2samp<br>from sklearn.ensemble import IsolationForest<br>from sklearn.model_selection import train_test_split<br>from sklearn.ensemble import RandomForestRegressor<br>from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer<br>from sklearn.model_selection import cross_val_score</pre>\n<pre>df = pd.read_csv('AIS_2024_01_01.csv',usecols=['MMSI', 'BaseDateTime', 'LAT', 'LON', 'SOG', 'COG', 'VesselType'], parse_dates=['BaseDateTime'])</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/674/1*FveZ0rxDl0CTE2GccVqBxA.png\"><figcaption>In the above dataframe, MMSI stands for Maritime Mobile Service Identity, which is a unique 9-digit number used to identify a ship. This is crucial for distinguishing different vessels in data. BaseDateTime is the timestamp of the AIS(Automatic Identification System) Signal, which is vital for understanding the sequence of vessel movements and for any time-series analysis or temporal patterns in vessel activity. LAT,LON (Latitude and Longitude) are the geographical coordinates of each AIS signal. These are essential for any spatial analysis, including tracking vessel routes or proximity to specific areas like ports. SOG stands for Speed Over Ground and it is the speed at which the vessel is moving over the bottom surface. This is a critical parameter for predicting vessel movement times and for identifying unusual patterns like speeding. COG (Course Over Ground) is the direction in which a vessel is moving, which, combined with SOG, helps in predicting the vessel\u2019s future location. VesselType indicates the type of vessel, which can affect its speed and allowable areas of operation. Different vessel types have different operational behaviors and constraints.</figcaption></figure><p>After loading the data, I checked for missing values and inconsistencies, which is a crucial step to ensure data quality. With a dataset of over 7 million rows, managing data efficiently is\u00a0key.</p>\n<h4>Code Snippet: Handling Missing\u00a0Values</h4>\n<pre>df.isnull().sum()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/217/1*yLyUu1eK2WRs-lcfC8xZrg.png\"></figure><pre>missing_vessel_type = df['VesselType'].isnull().sum()<br>total_rows = df.shape[0]<br>percent_missing = (missing_vessel_type / total_rows) * 100<br>print(f\"Missing 'VesselType' data: {percent_missing:.2f}%\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/565/1*gpvowNJd5m28DhV8swvDWw.png\"><figcaption>Since the entries for which the vessel type columns are missing a value make up a merely 0.1% of the overall dataset, we can remove the entries completely as we would still have enough data points to continue\u00a0with.</figcaption></figure><pre>df = df[df['VesselType'].isnull() == False]</pre>\n<h3>Step 2: Exploratory Data\u00a0Analysis</h3>\n<pre># Display summary statistics<br>df.describe()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/929/1*2gX4RuyzPk5i5j7Otm6iFw.png\"></figure><pre># Histograms for SOG and COG<br>plt.figure(figsize=(12, 6))<br>plt.subplot(1, 2, 1)<br>sns.histplot(df['SOG'], bins=30, kde=True)<br>plt.title('Distribution of Speed Over Ground (SOG)')<br><br>plt.subplot(1, 2, 2)<br>sns.histplot(df['COG'], bins=30, kde=True)<br>plt.title('Distribution of Course Over Ground (COG)')<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/970/1*Q_2OqCcmAKZY3uh34v4tRg.png\"></figure><pre>vessel_counts = df['VesselType'].value_counts()<br><br>plt.figure(figsize=(12, 8))  # Adjusting figure size<br>sns.barplot(y=vessel_counts.index, x=vessel_counts.values)  # Using a warmer color palette<br>plt.title('Distribution of Vessel Types', fontsize=16, fontweight='bold')  # Adding a title<br>plt.xlabel('Count', fontsize=14)  # Labeling the x-axis<br>plt.ylabel('Vessel Type', fontsize=14)  # Labeling the y-axis<br>plt.xticks(fontsize=8)  # Adjusting x-axis tick font size<br>plt.yticks(fontsize=8, rotation=0)  # Adjusting y-axis tick font size<br>plt.grid(axis='x', linestyle='--', alpha=0.7)  # Adding gridlines for better readability<br>plt.tight_layout()  # Adjust layout to not cut off labels<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/977/1*GE_FaSQ1nfIYJjKdTfsRug.png\"></figure><pre># Extract date and hour if not already done<br>df['Date'] = df['BaseDateTime'].dt.date<br>df['Hour'] = df['BaseDateTime'].dt.hour<br><br># Plotting traffic by hour of day<br>plt.figure(figsize=(10, 6))<br>sns.countplot(x='Hour', data=df)<br>plt.title('Vessel Traffic by Hour of Day')<br>plt.ylabel('Number of Records')<br>plt.xlabel('Hour of Day')<br>plt.xticks(rotation=45)<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/967/1*Kn4gmHXEqJ736NO7zUIRSw.png\"></figure><h3>Step 3: Feature Engineering</h3>\n<p>Feature engineering is one of the most impactful steps in any data science project. To represent vessel behavior accurately, I created new features from the timestamp, location, and also the vessel type data to help in the training process of the\u00a0Model.</p>\n<p>Mapping Vessel type from a dictionary mapping to convert numeric codes to descriptive labels.</p>\n<pre>vessel_type_mapping = {<br>    0: \"Not Available\",<br>    1: \"Reserved\",<br>    2: \"Reserved\",<br>    3: \"Reserved\",<br>    4: \"Reserved\",<br>      .<br>      .<br>      .<br>      .<br>      .<br>      .<br>      .<br>    20: \"Wing In Ground\",<br>    21: \"Wing In Ground (Hazardous)\",<br>    22: \"Wing In Ground (Hazardous Category A)\",<br>    23: \"Wing In Ground (Hazardous Category B)\",<br>      .<br>      .<br>      .<br>      .<br>      .<br><br>    89: \"Tanker (No Additional Information)\",<br>    90: \"Other Type\",<br>    91: \"Other Type (Hazardous Category A)\",<br>    92: \"Other Type (Hazardous Category B)\",<br>    93: \"Other Type (Hazardous Category C)\",<br>    94: \"Other Type (Hazardous Category D)\",<br>    99: \"Other Type (No Additional Information)\",<br>    255: \"Undefined\"<br>}</pre>\n<pre>df['VesselType'] = df['VesselType'].map(vessel_type_mapping)<br>df = df[df['VesselType']!= 'Undefined']</pre>\n<p>We also added additional columns such as \u2018SpeedDiff\u2019 and \u2018CourseDiff\u2019 that we could utilise for additional information relating to the change of speed and\u00a0course.</p>\n<pre># Sort data by vessel and time<br>df = df.sort_values(by=['MMSI', 'BaseDateTime'])<br><br># Calculate speed and course differences<br>df['SpeedDiff'] = df.groupby('MMSI')['SOG'].diff()  # Change in speed<br>df['CourseDiff'] = df.groupby('MMSI')['COG'].diff()  # Change in course</pre>\n<p>We then implemented 1 hot encoding for the categorical columns of Vessel Types so we could use them during the ML Training\u00a0process.</p>\n<pre>df_encoded = pd.get_dummies(df,columns=['VesselType'])<br>df_encoded['SpeedDiff'].fillna(0, inplace=True)<br>df_encoded['CourseDiff'].fillna(0, inplace=True)</pre>\n<p><strong>Data Aggregation and\u00a0Sampling</strong></p>\n<p>The goal of this step is to reduce the volume of data while preserving its core patterns. We\u2019ll achieve this by aggregating the data by time intervals for each vessel and sampling it to make it manageable for modeling.</p>\n<p>To start, we\u2019ll define an interval (such as every 5 minutes) to aggregate the data for each vessel. This helps us summarise the vessel\u2019s state at regular intervals instead of using every data point. You can adjust the interval based on the frequency of the data\u00a0points.</p>\n<pre># Convert 'BaseDateTime' to datetime if not already done<br>df_encoded['BaseDateTime'] = pd.to_datetime(df_encoded['BaseDateTime'])<br><br># Resample data every 5 minutes for each vessel<br>aggregation_interval = '5T'  # '5T' stands for 5 minutes<br><br># Identify all one-hot encoded vessel type columns<br>vessel_type_columns = [col for col in df_encoded.columns if col.startswith('VesselType_')]<br><br># Aggregation function dictionary with mean for each one-hot encoded column<br>aggregation_dict = {<br>    'LAT': 'mean',<br>    'LON': 'mean',<br>    'SOG': 'mean',<br>    'COG': 'mean',<br>    'SpeedDiff': 'mean',<br>    'CourseDiff': 'mean',<br>}<br><br># Add the one-hot encoded columns to aggregation dictionary with mean<br>for col in vessel_type_columns:<br>    aggregation_dict[col] = 'mean'<br><br># Convert 'BaseDateTime' to datetime if not already done<br>df_encoded['BaseDateTime'] = pd.to_datetime(df_encoded['BaseDateTime'])<br><br># Group by MMSI (vessel identifier) and resample every 5 minutes<br>df_aggregated = df_encoded.set_index('BaseDateTime').groupby('MMSI').resample(aggregation_interval).agg(aggregation_dict).reset_index()<br><br># Drop rows with NaN values, if any, after resampling<br>df_aggregated.dropna(inplace=True)</pre>\n<p>Even with aggregation, the dataset might still be large. We can apply sampling to reduce it further. However, to maintain proportional representation, we\u2019ll use <strong>stratified sampling</strong> by vessel\u00a0type.</p>\n<pre># Set the sampling fraction; for example, 10% of the data<br>sampling_fraction = 0.1<br><br># Stratified sampling by VesselType to ensure proportional representation<br>df_sampled = df_aggregated.groupby('VesselType', group_keys=False).apply(lambda x: x.sample(frac=sampling_fraction))</pre>\n<p>Once the data is aggregated and sampled, it\u2019s crucial to verify that the distributions still resemble the original data. You can do this by comparing the distributions of key features before and after sampling.</p>\n<pre># Plot comparison of SOG distribution<br>plt.figure(figsize=(10, 5))<br>sns.histplot(df_aggregated['SOG'], kde=True, color='skyblue', label='Aggregated')<br>sns.histplot(df_sampled['SOG'], kde=True, color='salmon', label='Sampled')<br>plt.legend()<br>plt.title(\"SOG Distribution: Aggregated vs Sampled\")<br>plt.xlabel(\"Speed Over Ground (SOG)\")<br>plt.ylabel(\"Density\")<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/916/1*b_YCOBw-l9zwOQONB9yVZQ.png\"></figure><p>Since the SOG values seem to be heavily skewed, applying a log transformation could help spread out the lower values and make the distribution more\u00a0visible.</p>\n<pre># Apply log transformation to SOG to spread out values<br>df_aggregated['log_SOG'] = np.log1p(df_aggregated['SOG'])  # log1p handles log(0) by adding 1<br>df_sampled['log_SOG'] = np.log1p(df_sampled['SOG'])<br><br># Plot comparison of log-transformed SOG distribution<br>plt.figure(figsize=(10, 5))<br>sns.histplot(df_aggregated['log_SOG'], kde=True, color='skyblue', label='Aggregated', bins=50)<br>sns.histplot(df_sampled['log_SOG'], kde=True, color='salmon', label='Sampled', bins=50)<br>plt.legend()<br>plt.title(\"Log-Transformed SOG Distribution: Aggregated vs Sampled\")<br>plt.xlabel(\"Log of Speed Over Ground (log_SOG)\")<br>plt.ylabel(\"Density\")<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/944/1*OXnH9wGJqgwmJHRgaw8vOw.png\"></figure><p>Looking at the log-transformed distribution of SOG (Speed Over Ground), we can evaluate whether the sampled data is representative of the aggregated data.</p>\n<p><em>Distribution Shape:</em> The sampled data closely follows the shape of the aggregated data in many regions, particularly around the main density peaks. This indicates that the sampling method has retained the overall distribution pattern, which is a positive sign for representativeness.</p>\n<p><em>Density Proportion:</em> Although the distribution shape is similar, the density values are much lower in the sampled data compared to the aggregated data. This is expected since we have reduced the dataset size, so the counts are lower, but the overall trends still\u00a0align.</p>\n<p><em>Critical Points:</em> There is a high-density peak around log_SOG close to 0 in the aggregated data, which is also reflected in the sampled data, though on a smaller scale. Other peaks around log_SOG values of approximately 1 to 2 are also captured in the\u00a0sample.</p>\n<p>We can further perform the Kolmogorov-Smirnov (KS) test to confirm our visual interpretation.</p>\n<pre># Perform the Kolmogorov-Smirnov test on the original and sampled SOG values<br>ks_stat, p_value = ks_2samp(df_aggregated['SOG'], df_sampled['SOG'])<br><br>print(f\"KS Statistic: {ks_stat}\")<br>print(f\"P-value: {p_value}\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/712/1*KttN7afok0K1FElw90Yzgg.png\"></figure><p>KS Statistic is 0.0024 (approximately), which is very close to 0. This suggests that there is minimal difference between the cumulative distributions of the original and sampled data for the SOG (Speed Over Ground) variable. P-value is 0.9061, which is significantly higher than the typical significance level of 0.05. This high p-value suggests that we do not have enough evidence to reject the null hypothesis that the two distributions are the\u00a0same.</p>\n<p>We also added additional columns such as hour, minute and second that can help us utilise the time information during the training process. We can later observe the feature importance of these columns after the model has been\u00a0trained.</p>\n<pre>df_sampled['Hour'] = df_sampled['BaseDateTime'].dt.hour<br>df_sampled['Minute'] = df_sampled['BaseDateTime'].dt.minute<br>df_sampled['Second'] = df_sampled['BaseDateTime'].dt.second</pre>\n<h3>Step 4: Anomaly Detection</h3>\n<p>Anomaly detection can be a powerful way to identify unexpected or risky behavior in vessel movement. Given the dataset, we can use features like SOG, COG to detect anomalies. Here, we\u2019ll try using <strong>Isolation Forest</strong>, a popular technique for anomaly detection.</p>\n<h4>A. Anomaly Detection Using Isolation Forest</h4>\n<p>The Isolation Forest algorithm works well with high-dimensional data and can isolate anomalies effectively.</p>\n<pre># Selecting features for anomaly detection<br>anomaly_features = df_sampled[['SOG', 'COG']]<br><br># Initialize and train the Isolation Forest model<br>isolation_forest = IsolationForest(contamination=0.01, random_state=42)  # Set contamination as per your tolerance level<br>df_sampled['anomaly'] = isolation_forest.fit_predict(anomaly_features)<br><br># Interpret results: -1 indicates an anomaly, 1 indicates normal<br>df_sampled['anomaly'] = df_sampled['anomaly'].apply(lambda x: 'Anomaly' if x == -1 else 'Normal')<br><br># Checking the distribution of anomalies<br>print(df_sampled['anomaly'].value_counts())</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/869/1*tjgijtS6_B_7Oc-X36G2iA.png\"></figure><h3>Step 5: Building a Predictive Model</h3>\n<p>In this step, we\u2019ll create a predictive model. For predicting fuel consumption or emissions, we can use a regression model that takes into account vessel features (such as vessel type), behavioral metrics (like speed and course), and other contextual factors.</p>\n<h4>A. Preparing Data for\u00a0Modeling</h4>\n<p>Prepare the features related to vessel operation, which could impact fuel consumption or emissions:</p>\n<ul>\n<li>\n<strong>Vessel Type</strong>: Different vessel types consume fuel at different rates.</li>\n<li>\n<strong>Speed Over Ground (SOG)</strong>: Fuel consumption increases with\u00a0speed.</li>\n<li>\n<strong>Course Over Ground (COG)</strong>: Not directly linked, but could be useful in conjunction with other variables.</li>\n<li>\n<strong>Geospatial Data (Latitude and Longitude)</strong>: For understanding location-based consumption, if relevant.</li>\n</ul>\n<p>Since I didn\u2019t have a real dataset for fuel consumption, I generated a synthetic target variable for demonstration purposes. Let\u2019s assume that fuel consumption is positively correlated with speed and varies across vessel\u00a0types.</p>\n<pre># Step 1: Feature Engineering<br># Add squared speed as a feature to account for non-linear relationship with fuel consumption<br>df_sampled['SOG_squared'] = df_sampled['SOG'] ** 2<br><br># Define features and target (synthetic fuel consumption)<br>features = ['LAT', 'LON', 'SOG', 'SOG_squared', 'COG', 'Hour', 'Minute', 'Second'] + vessel_type_columns<br><br># Generate synthetic target for demonstration purposes<br># Note: Replace this with actual fuel consumption data if available<br>np.random.seed(42)  # for reproducibility<br>df_sampled['fuel_consumption'] = 0.05 * df_sampled['SOG'] + 0.002 * df_sampled['SOG_squared'] + np.random.normal(0, 0.1, len(df_sampled))<br><br># Define X and y<br>X = df_sampled[features]<br>y = df_sampled['fuel_consumption']<br><br># Step 2: Split data into training and testing sets<br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)<br><br># Step 3: Initialize and train the model<br>rf_model = RandomForestRegressor(n_estimators=100, random_state=42)<br>rf_model.fit(X_train, y_train)<br><br># Step 4: Predict and evaluate the model<br>y_pred = rf_model.predict(X_test)<br><br># Calculate metrics<br>mse = mean_squared_error(y_test, y_pred)<br>mae = mean_absolute_error(y_test, y_pred)<br>print(\"Mean Squared Error:\", mse)<br>print(\"Mean Absolute Error:\", mae)<br><br># Step 5: Plotting Results<br>plt.figure(figsize=(10, 6))<br>sns.scatterplot(x=y_test, y=y_pred, alpha=0.3, color='blue')<br>plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)<br>plt.xlabel('Actual Fuel Consumption')<br>plt.ylabel('Predicted Fuel Consumption')<br>plt.title('Actual vs Predicted Fuel Consumption')<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/969/1*eNFtIiG-l5MY_rIoYzzZyQ.png\"></figure><p>The results in the plot indicate that our model is performing exceptionally well on the sampled dataset, with very low Mean Squared Error (MSE) and Mean Absolute Error (MAE). The points align closely with the red diagonal line, which represents a near perfect prediction, suggesting that the model\u2019s predictions for fuel consumption are almost identical to the actual values. Since we generated the fuel consumption variable synthetically using a mathematical function of the input features, the model could easily capture the relationship due to the lack of real-world noise and complexity. The synthetic target was directly dependent on SOG and SOG_squared, making it easier for the model to learn the pattern. The sampled data also may not represent the complexity of the full dataset. If the sampled data is relatively homogeneous, it could make predictions easier, especially for a powerful model like Random Forest that can capture intricate patterns in smaller datasets. There is a risk that the model may be overfitting the data if the patterns are too predictable. Random Forests, being an ensemble of decision trees, can sometimes overfit if the data has easily separable patterns.</p>\n<p>We will try to evaluate our results\u00a0further,</p>\n<h4>Cross-Validation</h4>\n<p>Cross-validation is a reliable way to check if our model\u2019s performance generalizes well. We\u2019ll perform k-fold cross-validation on the dataset and observe if the performance metrics remain consistent across\u00a0folds.</p>\n<pre># Define custom scoring function for MSE<br>mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)<br><br># Perform 5-fold cross-validation<br>cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring=mse_scorer)<br><br># Convert scores to positive and calculate mean and standard deviation<br>cv_scores = -cv_scores<br>print(\"Cross-Validation MSE Scores:\", cv_scores)<br>print(\"Mean CV MSE:\", np.mean(cv_scores))<br>print(\"Standard Deviation of CV MSE:\", np.std(cv_scores))</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/743/1*t1E9kE8OzXo-Foc4425S8g.png\"></figure><h4>Feature Importance Analysis</h4>\n<p>Examining feature importance helps us understand which features the model relies on the most. If a few features (e.g., SOG and SOG_squared) dominate, it could indicate that the model is simply learning a synthetic relationship.</p>\n<pre>importances = rf_model.feature_importances_<br>feature_names = X.columns<br><br># Display feature importance<br>feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})<br>feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)<br><br>print(feature_importance_df)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/918/1*vDqVfS9fqti5ES8B-Em46g.png\"></figure><p>SOG_squared and SOG features alone account for over 99% of the feature importance, indicating that the model is almost entirely dependent on speed-related features. This dominance suggests that the fuel consumption model is heavily influenced by vessel speed, which aligns with real-world expectations. However, such a strong dependence might also mean the model could struggle if there\u2019s a slight shift in the relationship between speed and fuel consumption. The minimal importance of VesselType suggests that it\u2019s contributing little information under the current configuration. It might be since our synthetic definition of calculating fuel consumption doesn\u2019t take into account the vessel type features such as size, efficiency etc.</p>\n<h4>Adding Noise to Target\u00a0Variable</h4>\n<pre># Add random noise to the target variable<br>noise = np.random.normal(0, 0.1, len(y_train))  # Adjust the standard deviation to control noise level<br>y_train_noisy = y_train + noise<br><br># Retrain the model with noisy data<br>rf_model_noisy = RandomForestRegressor(n_estimators=100, random_state=42)<br>rf_model_noisy.fit(X_train, y_train_noisy)<br><br># Predict and evaluate<br>y_pred_noisy = rf_model_noisy.predict(X_test)<br>print(\"MSE with Noisy Data:\", mean_squared_error(y_test, y_pred_noisy))<br>print(\"MAE with Noisy Data:\", mean_absolute_error(y_test, y_pred_noisy))</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/890/1*3lt_gbEcRlSYUdZthVmf8w.png\"></figure><h3>Conclusion and\u00a0Insights</h3>\n<p>This project has been a valuable learning experience, providing insights into vessel behavior, fuel consumption patterns, and feature engineering for maritime data. By aligning this project with the job description, I\u2019ve deepened my understanding of the role and demonstrated my ability to tackle real-world problems. Although this simulation of potential work is simplified, I primarily use this to demonstrate how with my problem solving skills and decision making when it comes to the steps involved in Data Analysis and ML model development, I can be an asset if given an opportunity to work in a team that values learning and taking challenges head on. That concludes my first post describing how I am undertaking the process of portfolio development now. Taking real challenges from real job descriptions.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=58a008076cdb\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>In today\u2019s competitive job market, applying for a job means more than just submitting a resume and cover letter. To truly stand out, it\u2019s essential to demonstrate real, hands-on skills that align with the role you\u2019re targeting. In my journey to apply for a data science position, I decided to go beyond the basics and develop a project that mirrors the responsibilities of the job description, enhancing my understanding of the role and showcasing my abilities.</p>\n<p>This article walks through my project step-by-step\u200a\u2014\u200afrom defining the problem, sourcing the data, performing exploratory data analysis, to building and validating a machine learning model. Whether I get the job or not, this method has provided me with invaluable experience, deeper insight into the role, and a practical showcase of my\u00a0skills.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*WK_iq9MKpTM7wZb5qmQZTw.png\"></figure><p>When applying for a data science position, especially in the maritime sector, I saw an opportunity to create a project focused on <strong>predicting fuel consumption for vessels</strong>. The goal was to use real-world data to model fuel consumption based on various features, such as vessel type, speed, and geolocation. This directly aligns with the job responsibilities, where predictive modeling and data analysis are\u00a0key.</p>\n<h3>Step 1: Setting Up the\u00a0Data</h3>\n<p>The first step in any data project is obtaining the right data. For this project, I used the <a href=\"https://coast.noaa.gov/\">NOAA dataset</a> that provides extensive records of vessel movements, speeds, and geolocation data. Here\u2019s how I approached this:</p>\n<h4>Code Snippet: Import Libraries and Load\u00a0Data</h4>\n<pre>import pandas as pd<br>import seaborn as sns<br>import matplotlib.pyplot as plt<br>import numpy as np<br>from scipy.stats import ks_2samp<br>from sklearn.ensemble import IsolationForest<br>from sklearn.model_selection import train_test_split<br>from sklearn.ensemble import RandomForestRegressor<br>from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer<br>from sklearn.model_selection import cross_val_score</pre>\n<pre>df = pd.read_csv('AIS_2024_01_01.csv',usecols=['MMSI', 'BaseDateTime', 'LAT', 'LON', 'SOG', 'COG', 'VesselType'], parse_dates=['BaseDateTime'])</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/674/1*FveZ0rxDl0CTE2GccVqBxA.png\"><figcaption>In the above dataframe, MMSI stands for Maritime Mobile Service Identity, which is a unique 9-digit number used to identify a ship. This is crucial for distinguishing different vessels in data. BaseDateTime is the timestamp of the AIS(Automatic Identification System) Signal, which is vital for understanding the sequence of vessel movements and for any time-series analysis or temporal patterns in vessel activity. LAT,LON (Latitude and Longitude) are the geographical coordinates of each AIS signal. These are essential for any spatial analysis, including tracking vessel routes or proximity to specific areas like ports. SOG stands for Speed Over Ground and it is the speed at which the vessel is moving over the bottom surface. This is a critical parameter for predicting vessel movement times and for identifying unusual patterns like speeding. COG (Course Over Ground) is the direction in which a vessel is moving, which, combined with SOG, helps in predicting the vessel\u2019s future location. VesselType indicates the type of vessel, which can affect its speed and allowable areas of operation. Different vessel types have different operational behaviors and constraints.</figcaption></figure><p>After loading the data, I checked for missing values and inconsistencies, which is a crucial step to ensure data quality. With a dataset of over 7 million rows, managing data efficiently is\u00a0key.</p>\n<h4>Code Snippet: Handling Missing\u00a0Values</h4>\n<pre>df.isnull().sum()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/217/1*yLyUu1eK2WRs-lcfC8xZrg.png\"></figure><pre>missing_vessel_type = df['VesselType'].isnull().sum()<br>total_rows = df.shape[0]<br>percent_missing = (missing_vessel_type / total_rows) * 100<br>print(f\"Missing 'VesselType' data: {percent_missing:.2f}%\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/565/1*gpvowNJd5m28DhV8swvDWw.png\"><figcaption>Since the entries for which the vessel type columns are missing a value make up a merely 0.1% of the overall dataset, we can remove the entries completely as we would still have enough data points to continue\u00a0with.</figcaption></figure><pre>df = df[df['VesselType'].isnull() == False]</pre>\n<h3>Step 2: Exploratory Data\u00a0Analysis</h3>\n<pre># Display summary statistics<br>df.describe()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/929/1*2gX4RuyzPk5i5j7Otm6iFw.png\"></figure><pre># Histograms for SOG and COG<br>plt.figure(figsize=(12, 6))<br>plt.subplot(1, 2, 1)<br>sns.histplot(df['SOG'], bins=30, kde=True)<br>plt.title('Distribution of Speed Over Ground (SOG)')<br><br>plt.subplot(1, 2, 2)<br>sns.histplot(df['COG'], bins=30, kde=True)<br>plt.title('Distribution of Course Over Ground (COG)')<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/970/1*Q_2OqCcmAKZY3uh34v4tRg.png\"></figure><pre>vessel_counts = df['VesselType'].value_counts()<br><br>plt.figure(figsize=(12, 8))  # Adjusting figure size<br>sns.barplot(y=vessel_counts.index, x=vessel_counts.values)  # Using a warmer color palette<br>plt.title('Distribution of Vessel Types', fontsize=16, fontweight='bold')  # Adding a title<br>plt.xlabel('Count', fontsize=14)  # Labeling the x-axis<br>plt.ylabel('Vessel Type', fontsize=14)  # Labeling the y-axis<br>plt.xticks(fontsize=8)  # Adjusting x-axis tick font size<br>plt.yticks(fontsize=8, rotation=0)  # Adjusting y-axis tick font size<br>plt.grid(axis='x', linestyle='--', alpha=0.7)  # Adding gridlines for better readability<br>plt.tight_layout()  # Adjust layout to not cut off labels<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/977/1*GE_FaSQ1nfIYJjKdTfsRug.png\"></figure><pre># Extract date and hour if not already done<br>df['Date'] = df['BaseDateTime'].dt.date<br>df['Hour'] = df['BaseDateTime'].dt.hour<br><br># Plotting traffic by hour of day<br>plt.figure(figsize=(10, 6))<br>sns.countplot(x='Hour', data=df)<br>plt.title('Vessel Traffic by Hour of Day')<br>plt.ylabel('Number of Records')<br>plt.xlabel('Hour of Day')<br>plt.xticks(rotation=45)<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/967/1*Kn4gmHXEqJ736NO7zUIRSw.png\"></figure><h3>Step 3: Feature Engineering</h3>\n<p>Feature engineering is one of the most impactful steps in any data science project. To represent vessel behavior accurately, I created new features from the timestamp, location, and also the vessel type data to help in the training process of the\u00a0Model.</p>\n<p>Mapping Vessel type from a dictionary mapping to convert numeric codes to descriptive labels.</p>\n<pre>vessel_type_mapping = {<br>    0: \"Not Available\",<br>    1: \"Reserved\",<br>    2: \"Reserved\",<br>    3: \"Reserved\",<br>    4: \"Reserved\",<br>      .<br>      .<br>      .<br>      .<br>      .<br>      .<br>      .<br>    20: \"Wing In Ground\",<br>    21: \"Wing In Ground (Hazardous)\",<br>    22: \"Wing In Ground (Hazardous Category A)\",<br>    23: \"Wing In Ground (Hazardous Category B)\",<br>      .<br>      .<br>      .<br>      .<br>      .<br><br>    89: \"Tanker (No Additional Information)\",<br>    90: \"Other Type\",<br>    91: \"Other Type (Hazardous Category A)\",<br>    92: \"Other Type (Hazardous Category B)\",<br>    93: \"Other Type (Hazardous Category C)\",<br>    94: \"Other Type (Hazardous Category D)\",<br>    99: \"Other Type (No Additional Information)\",<br>    255: \"Undefined\"<br>}</pre>\n<pre>df['VesselType'] = df['VesselType'].map(vessel_type_mapping)<br>df = df[df['VesselType']!= 'Undefined']</pre>\n<p>We also added additional columns such as \u2018SpeedDiff\u2019 and \u2018CourseDiff\u2019 that we could utilise for additional information relating to the change of speed and\u00a0course.</p>\n<pre># Sort data by vessel and time<br>df = df.sort_values(by=['MMSI', 'BaseDateTime'])<br><br># Calculate speed and course differences<br>df['SpeedDiff'] = df.groupby('MMSI')['SOG'].diff()  # Change in speed<br>df['CourseDiff'] = df.groupby('MMSI')['COG'].diff()  # Change in course</pre>\n<p>We then implemented 1 hot encoding for the categorical columns of Vessel Types so we could use them during the ML Training\u00a0process.</p>\n<pre>df_encoded = pd.get_dummies(df,columns=['VesselType'])<br>df_encoded['SpeedDiff'].fillna(0, inplace=True)<br>df_encoded['CourseDiff'].fillna(0, inplace=True)</pre>\n<p><strong>Data Aggregation and\u00a0Sampling</strong></p>\n<p>The goal of this step is to reduce the volume of data while preserving its core patterns. We\u2019ll achieve this by aggregating the data by time intervals for each vessel and sampling it to make it manageable for modeling.</p>\n<p>To start, we\u2019ll define an interval (such as every 5 minutes) to aggregate the data for each vessel. This helps us summarise the vessel\u2019s state at regular intervals instead of using every data point. You can adjust the interval based on the frequency of the data\u00a0points.</p>\n<pre># Convert 'BaseDateTime' to datetime if not already done<br>df_encoded['BaseDateTime'] = pd.to_datetime(df_encoded['BaseDateTime'])<br><br># Resample data every 5 minutes for each vessel<br>aggregation_interval = '5T'  # '5T' stands for 5 minutes<br><br># Identify all one-hot encoded vessel type columns<br>vessel_type_columns = [col for col in df_encoded.columns if col.startswith('VesselType_')]<br><br># Aggregation function dictionary with mean for each one-hot encoded column<br>aggregation_dict = {<br>    'LAT': 'mean',<br>    'LON': 'mean',<br>    'SOG': 'mean',<br>    'COG': 'mean',<br>    'SpeedDiff': 'mean',<br>    'CourseDiff': 'mean',<br>}<br><br># Add the one-hot encoded columns to aggregation dictionary with mean<br>for col in vessel_type_columns:<br>    aggregation_dict[col] = 'mean'<br><br># Convert 'BaseDateTime' to datetime if not already done<br>df_encoded['BaseDateTime'] = pd.to_datetime(df_encoded['BaseDateTime'])<br><br># Group by MMSI (vessel identifier) and resample every 5 minutes<br>df_aggregated = df_encoded.set_index('BaseDateTime').groupby('MMSI').resample(aggregation_interval).agg(aggregation_dict).reset_index()<br><br># Drop rows with NaN values, if any, after resampling<br>df_aggregated.dropna(inplace=True)</pre>\n<p>Even with aggregation, the dataset might still be large. We can apply sampling to reduce it further. However, to maintain proportional representation, we\u2019ll use <strong>stratified sampling</strong> by vessel\u00a0type.</p>\n<pre># Set the sampling fraction; for example, 10% of the data<br>sampling_fraction = 0.1<br><br># Stratified sampling by VesselType to ensure proportional representation<br>df_sampled = df_aggregated.groupby('VesselType', group_keys=False).apply(lambda x: x.sample(frac=sampling_fraction))</pre>\n<p>Once the data is aggregated and sampled, it\u2019s crucial to verify that the distributions still resemble the original data. You can do this by comparing the distributions of key features before and after sampling.</p>\n<pre># Plot comparison of SOG distribution<br>plt.figure(figsize=(10, 5))<br>sns.histplot(df_aggregated['SOG'], kde=True, color='skyblue', label='Aggregated')<br>sns.histplot(df_sampled['SOG'], kde=True, color='salmon', label='Sampled')<br>plt.legend()<br>plt.title(\"SOG Distribution: Aggregated vs Sampled\")<br>plt.xlabel(\"Speed Over Ground (SOG)\")<br>plt.ylabel(\"Density\")<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/916/1*b_YCOBw-l9zwOQONB9yVZQ.png\"></figure><p>Since the SOG values seem to be heavily skewed, applying a log transformation could help spread out the lower values and make the distribution more\u00a0visible.</p>\n<pre># Apply log transformation to SOG to spread out values<br>df_aggregated['log_SOG'] = np.log1p(df_aggregated['SOG'])  # log1p handles log(0) by adding 1<br>df_sampled['log_SOG'] = np.log1p(df_sampled['SOG'])<br><br># Plot comparison of log-transformed SOG distribution<br>plt.figure(figsize=(10, 5))<br>sns.histplot(df_aggregated['log_SOG'], kde=True, color='skyblue', label='Aggregated', bins=50)<br>sns.histplot(df_sampled['log_SOG'], kde=True, color='salmon', label='Sampled', bins=50)<br>plt.legend()<br>plt.title(\"Log-Transformed SOG Distribution: Aggregated vs Sampled\")<br>plt.xlabel(\"Log of Speed Over Ground (log_SOG)\")<br>plt.ylabel(\"Density\")<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/944/1*OXnH9wGJqgwmJHRgaw8vOw.png\"></figure><p>Looking at the log-transformed distribution of SOG (Speed Over Ground), we can evaluate whether the sampled data is representative of the aggregated data.</p>\n<p><em>Distribution Shape:</em> The sampled data closely follows the shape of the aggregated data in many regions, particularly around the main density peaks. This indicates that the sampling method has retained the overall distribution pattern, which is a positive sign for representativeness.</p>\n<p><em>Density Proportion:</em> Although the distribution shape is similar, the density values are much lower in the sampled data compared to the aggregated data. This is expected since we have reduced the dataset size, so the counts are lower, but the overall trends still\u00a0align.</p>\n<p><em>Critical Points:</em> There is a high-density peak around log_SOG close to 0 in the aggregated data, which is also reflected in the sampled data, though on a smaller scale. Other peaks around log_SOG values of approximately 1 to 2 are also captured in the\u00a0sample.</p>\n<p>We can further perform the Kolmogorov-Smirnov (KS) test to confirm our visual interpretation.</p>\n<pre># Perform the Kolmogorov-Smirnov test on the original and sampled SOG values<br>ks_stat, p_value = ks_2samp(df_aggregated['SOG'], df_sampled['SOG'])<br><br>print(f\"KS Statistic: {ks_stat}\")<br>print(f\"P-value: {p_value}\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/712/1*KttN7afok0K1FElw90Yzgg.png\"></figure><p>KS Statistic is 0.0024 (approximately), which is very close to 0. This suggests that there is minimal difference between the cumulative distributions of the original and sampled data for the SOG (Speed Over Ground) variable. P-value is 0.9061, which is significantly higher than the typical significance level of 0.05. This high p-value suggests that we do not have enough evidence to reject the null hypothesis that the two distributions are the\u00a0same.</p>\n<p>We also added additional columns such as hour, minute and second that can help us utilise the time information during the training process. We can later observe the feature importance of these columns after the model has been\u00a0trained.</p>\n<pre>df_sampled['Hour'] = df_sampled['BaseDateTime'].dt.hour<br>df_sampled['Minute'] = df_sampled['BaseDateTime'].dt.minute<br>df_sampled['Second'] = df_sampled['BaseDateTime'].dt.second</pre>\n<h3>Step 4: Anomaly Detection</h3>\n<p>Anomaly detection can be a powerful way to identify unexpected or risky behavior in vessel movement. Given the dataset, we can use features like SOG, COG to detect anomalies. Here, we\u2019ll try using <strong>Isolation Forest</strong>, a popular technique for anomaly detection.</p>\n<h4>A. Anomaly Detection Using Isolation Forest</h4>\n<p>The Isolation Forest algorithm works well with high-dimensional data and can isolate anomalies effectively.</p>\n<pre># Selecting features for anomaly detection<br>anomaly_features = df_sampled[['SOG', 'COG']]<br><br># Initialize and train the Isolation Forest model<br>isolation_forest = IsolationForest(contamination=0.01, random_state=42)  # Set contamination as per your tolerance level<br>df_sampled['anomaly'] = isolation_forest.fit_predict(anomaly_features)<br><br># Interpret results: -1 indicates an anomaly, 1 indicates normal<br>df_sampled['anomaly'] = df_sampled['anomaly'].apply(lambda x: 'Anomaly' if x == -1 else 'Normal')<br><br># Checking the distribution of anomalies<br>print(df_sampled['anomaly'].value_counts())</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/869/1*tjgijtS6_B_7Oc-X36G2iA.png\"></figure><h3>Step 5: Building a Predictive Model</h3>\n<p>In this step, we\u2019ll create a predictive model. For predicting fuel consumption or emissions, we can use a regression model that takes into account vessel features (such as vessel type), behavioral metrics (like speed and course), and other contextual factors.</p>\n<h4>A. Preparing Data for\u00a0Modeling</h4>\n<p>Prepare the features related to vessel operation, which could impact fuel consumption or emissions:</p>\n<ul>\n<li>\n<strong>Vessel Type</strong>: Different vessel types consume fuel at different rates.</li>\n<li>\n<strong>Speed Over Ground (SOG)</strong>: Fuel consumption increases with\u00a0speed.</li>\n<li>\n<strong>Course Over Ground (COG)</strong>: Not directly linked, but could be useful in conjunction with other variables.</li>\n<li>\n<strong>Geospatial Data (Latitude and Longitude)</strong>: For understanding location-based consumption, if relevant.</li>\n</ul>\n<p>Since I didn\u2019t have a real dataset for fuel consumption, I generated a synthetic target variable for demonstration purposes. Let\u2019s assume that fuel consumption is positively correlated with speed and varies across vessel\u00a0types.</p>\n<pre># Step 1: Feature Engineering<br># Add squared speed as a feature to account for non-linear relationship with fuel consumption<br>df_sampled['SOG_squared'] = df_sampled['SOG'] ** 2<br><br># Define features and target (synthetic fuel consumption)<br>features = ['LAT', 'LON', 'SOG', 'SOG_squared', 'COG', 'Hour', 'Minute', 'Second'] + vessel_type_columns<br><br># Generate synthetic target for demonstration purposes<br># Note: Replace this with actual fuel consumption data if available<br>np.random.seed(42)  # for reproducibility<br>df_sampled['fuel_consumption'] = 0.05 * df_sampled['SOG'] + 0.002 * df_sampled['SOG_squared'] + np.random.normal(0, 0.1, len(df_sampled))<br><br># Define X and y<br>X = df_sampled[features]<br>y = df_sampled['fuel_consumption']<br><br># Step 2: Split data into training and testing sets<br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)<br><br># Step 3: Initialize and train the model<br>rf_model = RandomForestRegressor(n_estimators=100, random_state=42)<br>rf_model.fit(X_train, y_train)<br><br># Step 4: Predict and evaluate the model<br>y_pred = rf_model.predict(X_test)<br><br># Calculate metrics<br>mse = mean_squared_error(y_test, y_pred)<br>mae = mean_absolute_error(y_test, y_pred)<br>print(\"Mean Squared Error:\", mse)<br>print(\"Mean Absolute Error:\", mae)<br><br># Step 5: Plotting Results<br>plt.figure(figsize=(10, 6))<br>sns.scatterplot(x=y_test, y=y_pred, alpha=0.3, color='blue')<br>plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)<br>plt.xlabel('Actual Fuel Consumption')<br>plt.ylabel('Predicted Fuel Consumption')<br>plt.title('Actual vs Predicted Fuel Consumption')<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/969/1*eNFtIiG-l5MY_rIoYzzZyQ.png\"></figure><p>The results in the plot indicate that our model is performing exceptionally well on the sampled dataset, with very low Mean Squared Error (MSE) and Mean Absolute Error (MAE). The points align closely with the red diagonal line, which represents a near perfect prediction, suggesting that the model\u2019s predictions for fuel consumption are almost identical to the actual values. Since we generated the fuel consumption variable synthetically using a mathematical function of the input features, the model could easily capture the relationship due to the lack of real-world noise and complexity. The synthetic target was directly dependent on SOG and SOG_squared, making it easier for the model to learn the pattern. The sampled data also may not represent the complexity of the full dataset. If the sampled data is relatively homogeneous, it could make predictions easier, especially for a powerful model like Random Forest that can capture intricate patterns in smaller datasets. There is a risk that the model may be overfitting the data if the patterns are too predictable. Random Forests, being an ensemble of decision trees, can sometimes overfit if the data has easily separable patterns.</p>\n<p>We will try to evaluate our results\u00a0further,</p>\n<h4>Cross-Validation</h4>\n<p>Cross-validation is a reliable way to check if our model\u2019s performance generalizes well. We\u2019ll perform k-fold cross-validation on the dataset and observe if the performance metrics remain consistent across\u00a0folds.</p>\n<pre># Define custom scoring function for MSE<br>mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)<br><br># Perform 5-fold cross-validation<br>cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring=mse_scorer)<br><br># Convert scores to positive and calculate mean and standard deviation<br>cv_scores = -cv_scores<br>print(\"Cross-Validation MSE Scores:\", cv_scores)<br>print(\"Mean CV MSE:\", np.mean(cv_scores))<br>print(\"Standard Deviation of CV MSE:\", np.std(cv_scores))</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/743/1*t1E9kE8OzXo-Foc4425S8g.png\"></figure><h4>Feature Importance Analysis</h4>\n<p>Examining feature importance helps us understand which features the model relies on the most. If a few features (e.g., SOG and SOG_squared) dominate, it could indicate that the model is simply learning a synthetic relationship.</p>\n<pre>importances = rf_model.feature_importances_<br>feature_names = X.columns<br><br># Display feature importance<br>feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})<br>feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)<br><br>print(feature_importance_df)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/918/1*vDqVfS9fqti5ES8B-Em46g.png\"></figure><p>SOG_squared and SOG features alone account for over 99% of the feature importance, indicating that the model is almost entirely dependent on speed-related features. This dominance suggests that the fuel consumption model is heavily influenced by vessel speed, which aligns with real-world expectations. However, such a strong dependence might also mean the model could struggle if there\u2019s a slight shift in the relationship between speed and fuel consumption. The minimal importance of VesselType suggests that it\u2019s contributing little information under the current configuration. It might be since our synthetic definition of calculating fuel consumption doesn\u2019t take into account the vessel type features such as size, efficiency etc.</p>\n<h4>Adding Noise to Target\u00a0Variable</h4>\n<pre># Add random noise to the target variable<br>noise = np.random.normal(0, 0.1, len(y_train))  # Adjust the standard deviation to control noise level<br>y_train_noisy = y_train + noise<br><br># Retrain the model with noisy data<br>rf_model_noisy = RandomForestRegressor(n_estimators=100, random_state=42)<br>rf_model_noisy.fit(X_train, y_train_noisy)<br><br># Predict and evaluate<br>y_pred_noisy = rf_model_noisy.predict(X_test)<br>print(\"MSE with Noisy Data:\", mean_squared_error(y_test, y_pred_noisy))<br>print(\"MAE with Noisy Data:\", mean_absolute_error(y_test, y_pred_noisy))</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/890/1*3lt_gbEcRlSYUdZthVmf8w.png\"></figure><h3>Conclusion and\u00a0Insights</h3>\n<p>This project has been a valuable learning experience, providing insights into vessel behavior, fuel consumption patterns, and feature engineering for maritime data. By aligning this project with the job description, I\u2019ve deepened my understanding of the role and demonstrated my ability to tackle real-world problems. Although this simulation of potential work is simplified, I primarily use this to demonstrate how with my problem solving skills and decision making when it comes to the steps involved in Data Analysis and ML model development, I can be an asset if given an opportunity to work in a team that values learning and taking challenges head on. That concludes my first post describing how I am undertaking the process of portfolio development now. Taking real challenges from real job descriptions.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=58a008076cdb\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["maritime-industry","random-forest","machine-learning","job-hunting","data-science"]},{"title":"Engineering a Smart Scheduling Algorithm for 450+ Cricket Matches Amidst Myriad Constraints (Part\u2026","pubDate":"2024-11-07 10:33:09","link":"https://medium.com/@HaveASesh/engineering-a-smart-scheduling-algorithm-for-450-cricket-matches-amidst-myriad-constraints-part-66c653250ff4?source=rss-ccd6c29405e0------2","guid":"https://medium.com/p/66c653250ff4","author":"HaveASesh","thumbnail":"","description":"\n<h3>Engineering a Smart Scheduling Algorithm for 450+ Cricket Matches Amidst Myriad Constraints (Part\u00a01)</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Ii1Il_ADltb804ZLTrH6sQ.png\"></figure><p>Scheduling matches for sports leagues can be a daunting task, especially when juggling multiple divisions, ground preferences, and a host of other logistical constraints. As a player in a couple of the said divisions, I noticed the limitations of manually produced schedule firsthand. I embarked on a mission to create an efficient, fair, and feasible match schedule using <strong>Pyomo</strong>, a powerful Python-based optimization modeling language. This article covers my iterative journey, highlighting the challenges faced, solutions devised, and the eventual triumph (hopefully) of crafting a comprehensive scheduling model.</p>\n<h3>1. The Challenge: Crafting a Comprehensive Scheduling Model</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*JvGnSlBPqGjELN2vtlz-SA.png\"></figure><p>Managing league matches involves balancing numerous constraints:</p>\n<ul><li>\n<strong>Divisions and Teams:</strong> I have started with 2 divisions(FPL and NLD 1), each with eight teams. I will continue adding further divisions and competitions to make the model more\u00a0complex.</li></ul>\n<p><strong>Ground Preferences:</strong></p>\n<ul>\n<li>FPL teams are restricted to play only at\u00a0<strong>Kerava</strong>.</li>\n<li>NLD 1 teams can play at any of the four grounds\u200a\u2014\u200a<strong>Kerava</strong>, <strong>Tikkurila</strong>, <strong>Rajakyl\u00e4</strong>, and <strong>K\u00e4pyl\u00e4</strong>\u200a\u2014\u200afollowing a strict preference order.</li>\n</ul>\n<p><strong>Scheduling Constraints:</strong></p>\n<ul>\n<li>Teams that have to travel long distances in each division must play only on weekends.</li>\n<li>Teams should not have more than two matches per week to prevent\u00a0fatigue.</li>\n<li>If a team has multiple matches on the same day, all matches must be scheduled at the same ground to avoid logistical nightmares.</li>\n</ul>\n<p><strong>Optimization Goal:</strong> Minimise the number of days required to complete all matches, ensuring a swift and efficient league progression and also avoid the biggest hurdle to Cricket in Finland, The\u00a0Winter.</p>\n<h3>2. Building the Foundation: The Initial Pyomo\u00a0Model</h3>\n<h4>A. Setting Up the Environment</h4>\n<p>I began by organising my data in an Excel file named schedule_data.xlsx, structured with two\u00a0sheets:</p>\n<p><strong>Teams Sheet:</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/319/1*BbBeWd5rCY24rJPCJoIoEA.png\"><figcaption>Teams Sheet</figcaption></figure><p><strong>Grounds Sheet:</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/650/1*OeUhdNXiVTZYNcGK5KEuGA.png\"><figcaption>Grounds Sheet</figcaption></figure><h4><strong>B. Crafting the Initial Pyomo\u00a0Model</strong></h4>\n<p>Using Pyomo, I defined sets for divisions, teams, dates, and grounds. The core of the model revolved around binary decision variables indicating whether a particular match is scheduled on a specific date and\u00a0ground.</p>\n<pre>model = ConcreteModel()<br><br># Sets<br>model.Divisions = Set(initialize=divisions)<br>model.Teams = Set(initialize=all_teams)<br>model.DivisionTeams = Set(model.Divisions, initialize=division_teams)<br>model.Dates = Set(initialize=dates_sorted)<br>model.Grounds = Set(initialize=grounds)<br>model.Matches = Set(dimen=3)  # (Division, HomeTeam, AwayTeam)<br><br># Flatten matches and associate with divisions<br>all_matches = []<br>for div in divisions:<br>    for match in matches[div]:<br>        all_matches.append((div, match[0], match[1]))<br>model.AllMatches = Set(initialize=all_matches, dimen=3)<br><br># Ground Availability Parameter<br>model.GroundAvailability = Param(model.Grounds, model.Dates, initialize=ground_capacity, default=0)<br><br># Determine if a date is weekend<br>date_weekend = {date: is_weekend(date) for date in dates_sorted}<br>model.IsWeekend = Param(model.Dates, initialize=date_weekend, within=Binary)<br><br># Ground preferences for NLD 1 (lower number means higher preference)<br>model.GroundPreference = Param(model.Grounds, initialize=ground_preferences, default=100)<br><br><br># Map each date to its ISO week number<br>date_to_week = {date: date.isocalendar()[1] for date in dates_sorted}<br><br># Create a set of unique weeks<br>weeks = sorted(set(date_to_week.values()))<br>model.Weeks = Set(initialize=weeks)<br><br># Parameter to map dates to weeks<br>model.DateToWeek = Param(model.Dates, initialize=date_to_week, within=NonNegativeIntegers)<br><br><br># Decision variable: y[div, h, a, d, g] = 1 if match is scheduled on date d at ground g<br>model.y = Var(model.AllMatches, model.Dates, model.Grounds, domain=Binary)<br><br># Auxiliary variable: u[t, d, g] = 1 if team t uses ground g on day d<br>model.u = Var(model.Teams, model.Dates, model.Grounds, domain=Binary)<br><br># Binary variable: z_day[d] = 1 if any match is scheduled on date d<br>model.z_day = Var(model.Dates, domain=Binary)</pre>\n<p>Key constraints included:</p>\n<p>Key constraints included:</p>\n<ol>\n<li>\n<strong>Match Scheduling:</strong> Each match must be scheduled exactly\u00a0once.</li>\n<li>\n<strong>Ground Capacity:</strong> Limits on the number of matches per ground per day, varying between weekends and weekdays.</li>\n<li>\n<strong>Team Scheduling:</strong> Ensuring teams don\u2019t exceed match limits per day and\u00a0week.</li>\n<li>\n<strong>Special Team Constraints:</strong> Special teams (such as the ones that travel from further away) could only play on weekends and had limitations on the number of matches per weekend\u00a0day.</li>\n</ol>\n<p>The initial model successfully scheduled matches while adhering to these constraints, but as league dynamics evolved, so did the complexity of the scheduling requirements.</p>\n<p>As the league expanded, the introduction of the <strong>NLD 1</strong> division brought new challenges:</p>\n<ul>\n<li>\n<strong>Ground Preferences:</strong> Unlike FPL, NLD 1 teams could play at any ground but had a clear preference hierarchy.</li>\n<li>\n<strong>Special Teams in NLD 1:</strong> Similar to FPL, NLD 1 had its special team with weekend-only match constraints.</li>\n</ul>\n<p>The output schedule is as shown\u00a0below:</p>\n<a href=\"https://medium.com/media/9c3c2a0209c70391f36092938e293980/href\">https://medium.com/media/9c3c2a0209c70391f36092938e293980/href</a><p>The optimization algorithm does a decent job in the initial stages however there needs to be further modifications, specifically in the number of matches each team plays for NLD1 similar to how FPL games have. I will continue fine tuning and increasing the complexity of the model in upcoming parts. Thanks for reading! Feel free to reach out to me if you have any questions or suggestions.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=66c653250ff4\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Engineering a Smart Scheduling Algorithm for 450+ Cricket Matches Amidst Myriad Constraints (Part\u00a01)</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Ii1Il_ADltb804ZLTrH6sQ.png\"></figure><p>Scheduling matches for sports leagues can be a daunting task, especially when juggling multiple divisions, ground preferences, and a host of other logistical constraints. As a player in a couple of the said divisions, I noticed the limitations of manually produced schedule firsthand. I embarked on a mission to create an efficient, fair, and feasible match schedule using <strong>Pyomo</strong>, a powerful Python-based optimization modeling language. This article covers my iterative journey, highlighting the challenges faced, solutions devised, and the eventual triumph (hopefully) of crafting a comprehensive scheduling model.</p>\n<h3>1. The Challenge: Crafting a Comprehensive Scheduling Model</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*JvGnSlBPqGjELN2vtlz-SA.png\"></figure><p>Managing league matches involves balancing numerous constraints:</p>\n<ul><li>\n<strong>Divisions and Teams:</strong> I have started with 2 divisions(FPL and NLD 1), each with eight teams. I will continue adding further divisions and competitions to make the model more\u00a0complex.</li></ul>\n<p><strong>Ground Preferences:</strong></p>\n<ul>\n<li>FPL teams are restricted to play only at\u00a0<strong>Kerava</strong>.</li>\n<li>NLD 1 teams can play at any of the four grounds\u200a\u2014\u200a<strong>Kerava</strong>, <strong>Tikkurila</strong>, <strong>Rajakyl\u00e4</strong>, and <strong>K\u00e4pyl\u00e4</strong>\u200a\u2014\u200afollowing a strict preference order.</li>\n</ul>\n<p><strong>Scheduling Constraints:</strong></p>\n<ul>\n<li>Teams that have to travel long distances in each division must play only on weekends.</li>\n<li>Teams should not have more than two matches per week to prevent\u00a0fatigue.</li>\n<li>If a team has multiple matches on the same day, all matches must be scheduled at the same ground to avoid logistical nightmares.</li>\n</ul>\n<p><strong>Optimization Goal:</strong> Minimise the number of days required to complete all matches, ensuring a swift and efficient league progression and also avoid the biggest hurdle to Cricket in Finland, The\u00a0Winter.</p>\n<h3>2. Building the Foundation: The Initial Pyomo\u00a0Model</h3>\n<h4>A. Setting Up the Environment</h4>\n<p>I began by organising my data in an Excel file named schedule_data.xlsx, structured with two\u00a0sheets:</p>\n<p><strong>Teams Sheet:</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/319/1*BbBeWd5rCY24rJPCJoIoEA.png\"><figcaption>Teams Sheet</figcaption></figure><p><strong>Grounds Sheet:</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/650/1*OeUhdNXiVTZYNcGK5KEuGA.png\"><figcaption>Grounds Sheet</figcaption></figure><h4><strong>B. Crafting the Initial Pyomo\u00a0Model</strong></h4>\n<p>Using Pyomo, I defined sets for divisions, teams, dates, and grounds. The core of the model revolved around binary decision variables indicating whether a particular match is scheduled on a specific date and\u00a0ground.</p>\n<pre>model = ConcreteModel()<br><br># Sets<br>model.Divisions = Set(initialize=divisions)<br>model.Teams = Set(initialize=all_teams)<br>model.DivisionTeams = Set(model.Divisions, initialize=division_teams)<br>model.Dates = Set(initialize=dates_sorted)<br>model.Grounds = Set(initialize=grounds)<br>model.Matches = Set(dimen=3)  # (Division, HomeTeam, AwayTeam)<br><br># Flatten matches and associate with divisions<br>all_matches = []<br>for div in divisions:<br>    for match in matches[div]:<br>        all_matches.append((div, match[0], match[1]))<br>model.AllMatches = Set(initialize=all_matches, dimen=3)<br><br># Ground Availability Parameter<br>model.GroundAvailability = Param(model.Grounds, model.Dates, initialize=ground_capacity, default=0)<br><br># Determine if a date is weekend<br>date_weekend = {date: is_weekend(date) for date in dates_sorted}<br>model.IsWeekend = Param(model.Dates, initialize=date_weekend, within=Binary)<br><br># Ground preferences for NLD 1 (lower number means higher preference)<br>model.GroundPreference = Param(model.Grounds, initialize=ground_preferences, default=100)<br><br><br># Map each date to its ISO week number<br>date_to_week = {date: date.isocalendar()[1] for date in dates_sorted}<br><br># Create a set of unique weeks<br>weeks = sorted(set(date_to_week.values()))<br>model.Weeks = Set(initialize=weeks)<br><br># Parameter to map dates to weeks<br>model.DateToWeek = Param(model.Dates, initialize=date_to_week, within=NonNegativeIntegers)<br><br><br># Decision variable: y[div, h, a, d, g] = 1 if match is scheduled on date d at ground g<br>model.y = Var(model.AllMatches, model.Dates, model.Grounds, domain=Binary)<br><br># Auxiliary variable: u[t, d, g] = 1 if team t uses ground g on day d<br>model.u = Var(model.Teams, model.Dates, model.Grounds, domain=Binary)<br><br># Binary variable: z_day[d] = 1 if any match is scheduled on date d<br>model.z_day = Var(model.Dates, domain=Binary)</pre>\n<p>Key constraints included:</p>\n<p>Key constraints included:</p>\n<ol>\n<li>\n<strong>Match Scheduling:</strong> Each match must be scheduled exactly\u00a0once.</li>\n<li>\n<strong>Ground Capacity:</strong> Limits on the number of matches per ground per day, varying between weekends and weekdays.</li>\n<li>\n<strong>Team Scheduling:</strong> Ensuring teams don\u2019t exceed match limits per day and\u00a0week.</li>\n<li>\n<strong>Special Team Constraints:</strong> Special teams (such as the ones that travel from further away) could only play on weekends and had limitations on the number of matches per weekend\u00a0day.</li>\n</ol>\n<p>The initial model successfully scheduled matches while adhering to these constraints, but as league dynamics evolved, so did the complexity of the scheduling requirements.</p>\n<p>As the league expanded, the introduction of the <strong>NLD 1</strong> division brought new challenges:</p>\n<ul>\n<li>\n<strong>Ground Preferences:</strong> Unlike FPL, NLD 1 teams could play at any ground but had a clear preference hierarchy.</li>\n<li>\n<strong>Special Teams in NLD 1:</strong> Similar to FPL, NLD 1 had its special team with weekend-only match constraints.</li>\n</ul>\n<p>The output schedule is as shown\u00a0below:</p>\n<a href=\"https://medium.com/media/9c3c2a0209c70391f36092938e293980/href\">https://medium.com/media/9c3c2a0209c70391f36092938e293980/href</a><p>The optimization algorithm does a decent job in the initial stages however there needs to be further modifications, specifically in the number of matches each team plays for NLD1 similar to how FPL games have. I will continue fine tuning and increasing the complexity of the model in upcoming parts. Thanks for reading! Feel free to reach out to me if you have any questions or suggestions.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=66c653250ff4\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["cricket","league","scheduling","optimization"]}]}